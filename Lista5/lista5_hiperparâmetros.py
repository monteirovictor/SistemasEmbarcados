# -*- coding: utf-8 -*-
"""Lista5-Hiperparâmetros.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/10UcaPrxlL-AlXQeb7umzjfjuy24Q7d9o

# Primeiro Teste

# Análise dos dados, sem nenhuma técnica de pré-processamento
"""

import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
from sklearn.model_selection import train_test_split

#importação excel
df=pd.read_excel("/content/drive/MyDrive/python/dataset-acelerometro.xlsx")

#visualizando o df
df.head()

#dados estatísticos do df
df.describe()

#Classificação KNN

from sklearn.neighbors import KNeighborsClassifier

X = df[['x', 'y', 'z']]
y = df['movimento']
print(X.shape)
print(y.shape)

print(df.Sensor.unique())

print(np.bincount(y))
print(df.Sensor.value_counts())
print('\n# a: '+str(len(df.loc[df['Sensor'] == 'a'])))

X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)
knn = KNeighborsClassifier(n_neighbors = 5)
knn.fit(X_train, y_train)
print("KNN accuracy no teste: ", knn.score(X_test, y_test))

pd.crosstab(y, knn.predict(X), colnames=['Classificação'])

#matriz confusão
from sklearn.metrics import plot_confusion_matrix
plot_confusion_matrix(knn, X, y,values_format='d',cmap=plt.cm.Blues)

#informações da classificação

from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
y_pred = knn.predict(X_test)
print("Relatório de classificação: \n", classification_report(y_test, y_pred))

"""# Segundo Teste

# Análise dos dados, removendo outliers

"""

#verificando os outliers

x=df['x']
y=df['y']
z=df['z']
boxplot=[x,y,z]

plt.boxplot(boxplot)
plt.xticks([1, 2, 3], ['x', 'y', 'z'])
plt.show()

#Tratamento dos Outliers para os valores de X

#Calculando os Quartis
percentile25 = df['x'].quantile(0.25)
percentile75 = df['x'].quantile(0.75)

#Amplitude Interquartílica

iqr=percentile75-percentile25

#Limite superior e Limite inferior
upper_limit = percentile75 + 1.5 * iqr
lower_limit = percentile25 - 1.5 * iqr

#Buscando os Outliers
df[df['x'] > upper_limit]
df[df['x'] < lower_limit]

#aparando os dados

new_df = df[df['x'] < upper_limit]
new_df.shape

new_df_cap = df.copy()
new_df_cap['x'] = np.where(
    new_df_cap['x'] > upper_limit,
    upper_limit,
    np.where(
        new_df_cap['x'] < lower_limit,
        lower_limit,
        new_df_cap['x']
    )
)

#Tratamento dos Outliers para os valores de y

#Calculando os Quartis
percentiley25 = df['y'].quantile(0.25)
percentiley75 = df['y'].quantile(0.75)

#Amplitude Interquartílica y

iqry=percentiley75-percentiley25

#Limite superior e Limite inferior
upper_limity = percentiley75 + 1.5 * iqry
lower_limity = percentiley25 - 1.5 * iqry

#Buscando os Outliers
df[df['y'] > upper_limity]
df[df['y'] < lower_limity]

#aparando os dados

new_dfy = df[df['y'] < upper_limity]
new_dfy.shape

new_df_capy = df.copy()
new_df_capy['y'] = np.where(
    new_df_capy['y'] > upper_limity,
    upper_limity,
    np.where(
        new_df_capy['y'] < lower_limity,
        lower_limity,
        new_df_capy['y']
    )
)

#Tratamento dos Outliers para os valores de z

#Calculando os Quartis
percentilez25 = df['z'].quantile(0.25)
percentilez75 = df['z'].quantile(0.75)

#Amplitude Interquartílica

iqrz=percentilez75-percentilez25

#Limite superior e Limite inferior
upper_limitz = percentilez75 + 1.5 * iqrz
lower_limitz = percentilez25 - 1.5 * iqrz

#Buscando os Outliers
df[df['z'] > upper_limitz]
df[df['z'] < lower_limitz]

#aparando os dados

new_dfz = df[df['z'] < upper_limitz]
new_dfz.shape

new_df_capz = df.copy()
new_df_capz['z'] = np.where(
    new_df_capz['z'] > upper_limitz,
    upper_limitz,
    np.where(
        new_df_capz['z'] < lower_limitz,
        lower_limitz,
        new_df_capz['z']
    )
)



#Outliers Tratados

xOutliers=new_df_cap['x']
yOutliers=new_df_capy['y']
zOutliers=new_df_capz['z']
outliers_tratados=[xOutliers,yOutliers,zOutliers]
plt.boxplot(outliers_tratados)
plt.xticks([1, 2, 3], ['x', 'y', 'z'])
plt.show()

#Montagem de um novo dataframe utlizando os x,y,z tratados,pegando as colunas
#indices,sensor,movimento do df original 

indice=df['indice']
Tempo=df['Tempo']
Sensor=df['Sensor']
movimento=df['movimento']

dataframe_outliers = pd.DataFrame(list(zip(indice,xOutliers,yOutliers,zOutliers,Tempo,Sensor,movimento)),columns=['indice','x', 'y', 'z','Tempo','Sensor','movimento'])
dataframe_outliers

#classificação KNN

from sklearn.neighbors import KNeighborsClassifier

X = dataframe_outliers[['x', 'y', 'z']]
y = dataframe_outliers['movimento']
print(X.shape)
print(y.shape)

print(dataframe_outliers.Sensor.unique())

print(np.bincount(y))
print(dataframe_outliers.Sensor.value_counts())
print('\n# a: '+str(len(dataframe_outliers.loc[dataframe_outliers['Sensor'] == 'a'])))

X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)
knn = KNeighborsClassifier(n_neighbors = 5)
knn.fit(X_train, y_train)
print("KNN accuracy no teste: ", knn.score(X_test, y_test))

pd.crosstab(y, knn.predict(X), colnames=['Predita'])

#matriz confusão
from sklearn.metrics import plot_confusion_matrix
plot_confusion_matrix(knn, X, y,values_format='d',cmap=plt.cm.Blues)

#informações da classificação

from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
y_pred = knn.predict(X_test)
print("Relatório de classificação: \n", classification_report(y_test, y_pred))

"""# Terceiro Teste 

#Normalização



"""

x=df['x']
y=df['y']
z=df['z']

df_n = pd.DataFrame(list(zip(x,y,z)), columns = ['x','y','z'])
from sklearn.preprocessing import MinMaxScaler
df_n.head()

# treina o algoritmo e cria o objeto obj_norm
obj_norm = MinMaxScaler().fit(df_n)

# aplica o normalizador
norm_df = obj_norm.transform(df_n)

# imprime os dados normalizados
norm_df

norm_df = pd.DataFrame(norm_df)
norm_df

ind=df['indice']
x=norm_df[0]
y=norm_df[1]
z=norm_df[2]
Tempo=df['Tempo']
sensor=df['Sensor']
movimento=df['movimento']


df_final = pd.DataFrame(list(zip(ind,x,y,z,Tempo,sensor,movimento)), columns = ['indice','x','y','z','Tempo','Sensor','movimento'])

df_final

#Classificação KNN

from sklearn.neighbors import KNeighborsClassifier

X = df_final[['x', 'y', 'z']]
y = df_final['movimento']
print(X.shape)
print(y.shape)

print(df_final.Sensor.unique())

print(np.bincount(y))
print(df_final.Sensor.value_counts())
print('\n# a: '+str(len(df_final.loc[df_final['Sensor'] == 'a'])))

X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)
knn = KNeighborsClassifier(n_neighbors = 5)
knn.fit(X_train, y_train)
print("KNN accuracy no teste: ", knn.score(X_test, y_test))

pd.crosstab(y, knn.predict(X), colnames=['Predita'])

#matriz confusão
from sklearn.metrics import plot_confusion_matrix
plot_confusion_matrix(knn, X, y,values_format='d',cmap=plt.cm.Blues)

#informações da classificação

from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
y_pred = knn.predict(X_test)
print("Relatório de classificação: \n", classification_report(y_test, y_pred))

"""# Quarto Teste 

#Técnica da Janela

"""

rolling = df.rolling(window=10)
rolling_mean = rolling.mean()
rolling_mean = rolling_mean.dropna()
rolling_mean

#Classificação KNN


X = rolling_mean[['x', 'y', 'z']]
y = rolling_mean.movimento.astype(int)

X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)

knn = KNeighborsClassifier(n_neighbors = 5)
knn.fit(X_train, y_train)

print("KNN accuracy no teste: ", knn.score(X_test, y_test))

pd.crosstab(y, knn.predict(X), colnames=['Classificação'])

plot_confusion_matrix(knn, X, y, values_format = 'd',cmap=plt.cm.Blues)

y_pred = knn.predict(X_test)
print("Relatório de classificação: \n", classification_report(y_test, y_pred))

"""# Hiperparâmetros


---

São parâmetros que não são aprendidos diretamente pelos modelo, portanto precisam ser imputados antes
do treinamento, sendo passado como argumentos de forma a influenciar na qualidade dos modelos, bem como 
na capacidade de aprendizagem com o treinamento e de generalizar para os dados não vistos em processos
anteriores.

# Classificação KNN
"""

X = rolling_mean[['x', 'y', 'z']]
y = rolling_mean.movimento.astype(int)

X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)

knn3 = KNeighborsClassifier(n_neighbors = 3)
knn3.fit(X_train, y_train)

print("KNN - acurácia com parametro '3': ", knn3.score(X_test, y_test))

X = rolling_mean[['x', 'y', 'z']]
y = rolling_mean.movimento.astype(int)

X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)

knn5 = KNeighborsClassifier(n_neighbors = 5)
knn5.fit(X_train, y_train)

print("KNN - acurácia com parametro '5': ", knn5.score(X_test, y_test))

X = rolling_mean[['x', 'y', 'z']]
y = rolling_mean.movimento.astype(int)

X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)

knn150 = KNeighborsClassifier(n_neighbors = 150)
knn150.fit(X_train, y_train)

print("KNN - acurácia com parametro '150': ", knn150.score(X_test, y_test))

pd.crosstab(y, knn5.predict(X), colnames=['Classificação'])

plot_confusion_matrix(knn5, X_test, y_pred, values_format = 'd',cmap=plt.cm.Blues)

y_pred = knn5.predict(X_test)
print("Classificação da melhor acurácia: \n", classification_report(y_test, y_pred))

"""#Classificação - SVC (Support Vector Machines)

SVM é um algoritmo de aprendizado de máquina supervisionado que pode ser usado para desafios de classificação ou regressão.
Aplica-se na resolução de problemas lineares e não lineares, funciona bem para muitos problemas práticos.
A ideia do SVM é simples: o algoritmo cria uma linha ou um hiperplano que separa os dados em classes.

Prós:

- Funciona muito bem com margem de separação clara.
- É eficaz nos casos em que o número de dimensões é maior que o número
de amostras.
- Ele usa um subconjunto de pontos de treinamento na função de decisão
(chamados de vetores de suporte), portanto, também é eficiente em
termos de memória.

Contras:

- Não tem um bom desempenho quando temos um grande conjunto de
dados porque o tempo de treinamento necessário é grande.
- Ele também não funciona muito bem quando o conjunto de dados tem
mais ruído, ou seja, as classes de destino estão sobrepostas.

https://www.codigofluente.com.br/aula-08-scikit-learn-maquina-de-vetores-de-suporte/

https://www.inf.ufpr.br/dagoncalves/IA07.pdf
"""

from IPython.display import Image
Image("/content/drive/MyDrive/python/SVM.png")

from sklearn.svm import SVC
from sklearn.metrics import classification_report, confusion_matrix

svc3 = SVC(C=3, gamma=0.1).fit(X_train, y_train)
svc3.score(X_test, y_test)

y_pred = svc3.predict(X_test)
print("SVC - acurácia com parametro C = '3': ", svc3.score(X_test, y_test))

svc5 = SVC(C=5, gamma=0.1).fit(X_train, y_train)
svc5.score(X_test, y_test)

y_pred = svc5.predict(X_test)
print("SVC - acurácia com parametro C = '5': ", svc5.score(X_test, y_test))

svc150 = SVC(C=150, gamma=0.1).fit(X_train, y_train)
svc150.score(X_test, y_test)

y_pred = svc150.predict(X_test)
print("SVC - acurácia com parametro C = '150': ", svc150.score(X_test, y_test))

y_pred = svc150.predict(X_test)

print("Classificação da melhor acurácia: \n",classification_report(y_test, y_pred))
plot_confusion_matrix(svc150, X_test, y_test, values_format = 'd',cmap=plt.cm.Blues)

"""# Classificação com Random Forrest


Random significa aleatório, e denota o comportamento do algoritmo ao selecionar subconjuntos de features e 
montar mini árvores de decisão. Forest significa floresta, já que são geradas várias árvores de decisão.

Por que devemos utilizar o Random Forest?

Pois uma árvore com grande profundidade tende a responder muito bem a dados conhecidos, 
porém perca poder de generalização e responda mal a dados desconhecidos(o famoso e temido overfitting).

Funcionamento do Random Forest

1. Seleção aleatória de algumas features
2. Seleção da feature mais adequada para a posição de nó raiz
3. Geração dos nós filhos
4. Repete os passos acima até que se atinja a quantidade de árvores desejada

https://medium.com/cinthiabpessanha/random-forest-como-funciona-um-dos-algoritmos-mais-populares-de-ml-cc1b8a58b3b4



"""

from IPython.display import Image
Image("/content/drive/MyDrive/python/arvore.png")

from IPython.display import Image
Image("/content/drive/MyDrive/python/random.png")

from sklearn.model_selection import cross_val_score
from sklearn.ensemble import RandomForestClassifier

rf = RandomForestClassifier(n_estimators=3,random_state=0)
cv_scores3 = cross_val_score(rf, X, y, cv=10)
print('Mean cross-validation score with 10 folds: {:.3f}'.format(np.mean(cv_scores3)))

rf = RandomForestClassifier(n_estimators=5,random_state=0)
cv_scores5 = cross_val_score(rf, X, y, cv=10)
print('Mean cross-validation score with 10 folds: {:.3f}'.format(np.mean(cv_scores5)))

rf = RandomForestClassifier(n_estimators=150,random_state=0)
cv_scores150 = cross_val_score(rf, X, y, cv=10)
print('Mean cross-validation score with 10 folds: {:.3f}'.format(np.mean(cv_scores150)))